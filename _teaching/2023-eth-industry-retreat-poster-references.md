---
title: "Poster References"
collection: teaching
type: "ETH Industry Retreat"
permalink: /teaching/2023-eth-industry-retreat-poster-references
venue: "Switzerland"
date: 2023-01-11
location: ""
---

# References

[1] Fedus, W., Zoph, B., & Shazeer, N. (2021). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. [https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf](https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf)  
  
[2] Rajbhandari, S., Li, C., Yao, Z., Zhang, M., Aminabadi, R. Y., Awan, A. A., ... & He, Y. (2022). Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale. arXiv preprint arXiv:2201.05596. [https://arxiv.org/abs/2201.05596](https://arxiv.org/abs/2201.05596)  

## Related Work
[3] Chen, C., Li, M., Wu, Z., Yu, D., & Yang, C. TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training. In Advances in Neural Information Processing Systems. [https://openreview.net/pdf?id=FRDiimH26Tr](https://openreview.net/pdf?id=FRDiimH26Tr)
      
[4] He, J., Zhai, J., Antunes, T., Wang, H., Luo, F., Shi, S., & Li, Q. (2022, April). FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (pp. 120-134). [https://dl.acm.org/doi/abs/10.1145/3503221.3508418](https://dl.acm.org/doi/abs/10.1145/3503221.3508418)  